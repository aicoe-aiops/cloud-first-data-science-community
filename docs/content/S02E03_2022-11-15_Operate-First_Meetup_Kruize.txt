Thorsten    00:00:04    Welcome to our biweekly Operate first Community Cloud Meetup. It is again Tuesday, and it's again, 11:00 AM Eastern Time slash New York. We are recording this meeting to give everybody a chance to follow. You can find our privacy policies at OP one st slash privacy policy. This meetup is held under the Open Info Foundation Code of Conduct, which you can find at OP one st slash Community Code of Conduct. Look up the details about this meeting at op onet slash meetup. This meetup is a gathering point for practices, project lessons learned, help needed, and ideas on and around the Operate First idea in general, and the Operate First Community Cloud in particular, this meeting can be as interactive as needed, or call it the Hands on meeting. This is the third episode of Season two, and I'm more than happy to welcome the Cruise Project, cruise Auto Tune project here in this meeting led by Rashmi GaN, and she will introduce all the others as follows. So a really warm welcome to Rashmi.  
Rashmi    00:01:33    Thanks, Toon. Uh, thank you everyone. Uh, so my name is Rashmi. Uh, I happen to make a couple of you guys before one of the presentations. So I'm the manager for the dream cruise, and I love the introduction that you put up in the community called, uh, it's exactly what we are doing. So we are cruising through the Kubernetes application, so it's cruise with the K, um, and I'm joined by my team, my architect, Ker, uh, my engineer, Seth, and my QE architect Chandra. So Chandra would be talking about a gross hpo, which we are working on deploying on first. So, yeah, what to you guys?  
Chandrakala    00:02:15    Thanks. Can you share your, yeah,  
Saad    00:02:26    <affirmative>,  
Chandrakala    00:02:34    Uh, hi everyone. Uh, as <inaudible> has already introduced, so, and this get straightaway started, uh, pro's, HBO is one of the projects under, uh, pro's umbrella. So we'll see a lot of projects under it, uh, after a little while. So, uh, this is the agenda, brief agenda where we would start off with the challenges and performance testing, followed by brief introduction to BA in optimization. And we look at the Bruce HPO architecture, and so will take us to the HBO demo, and we'll look at some of the experiment results we have had so far. Uh, next slide. Yeah, so imagine you are an SRE and you've been given the task of, uh, during an application performance. So if you look at any application, we see there are several layers starting from if it's a containerized application, you see the container layer, your frameworks, and where, and the runtime layer too.  
Chandrakala    00:03:28    And each of these layers have several unables. So for example, if you consider, uh, the Java runtime itself, it has hundreds of unables, and, uh, there's a warning there. Do not touch these knobs because we don't know tuning one of the unable, uh, what impact it would have on the other universe. And given that there are so many during them manually is quite cumbersome. And, uh, given that there are so many unable choices are never this simple. So, uh, let's go to the next slide and next slide. Sad. So fixing the hyper parameters first. So you must have already heard about this, for example, while building an ML model. Two, we look at fixing the hyper parameters because these are the parameters that control the learning process. So here we are applying this for, uh, tuning the performance of an application where we are looking at various unables at various layers, and, uh, trying to fix these hyper parameters first.  
Chandrakala    00:04:29    So the various options that we have to fix these hyper parameters are we could go do a manual search, and we could also do, we have grid search and random search, but then we have looked at various studies and found that basing optimization, uh, benefit fits here where we have several unables. And this would help us, uh, arrive at the optimal configuration, uh, in a much, uh, faster pace when compared to grid search and random search. So this is a brief introduction to basion optimization. So if we look at, uh, what is basion optimization all about? So basing optimization is a black box optimization technique where we feed the objective function that we need to either maximize or minimize, and we also need to freedom all at unables, which along with the rangers in which they operate, and both these together, they form the third space.  
Chandrakala    00:05:26    So if you look at the graphs there, we see the objective function on the Y axis and all that unables on the X axis, which forms the third space. So, uh, basing optimization, uh, gives us a query point, which is a red dot there, drawn out of this third space. And we use this, uh, each query point there correspond to a trial conflict that we apply to the application, conduct the experiments, run our benchmark application, compute the objective function, and then we again, golo it, which are those black points there on the graphs. So as in how we, uh, try to build, and based on the result that we have plotted, bian, again, looks at the result to, to predict the next trial conflict. So it, based on the result that we get, it draws the next sample, uh, and gives the next trial conflict.  
Chandrakala    00:06:17    So using this trial conflict, again, we connect the experiment and similarly, uh, go on compute the objective function and plot and based in optimization here uses this acquisition function, the green lines where you see, and in the third, uh, graph, you've seen that, uh, the moment it has hit the lower, uh, point that, uh, it, it goes back between, uh, draws another sample, which is between the higher two points that it has found, and it tries to exploit the area where it is more promising and around the local maximum there. So as in how, uh, it has completed exploiting a particular Earth, earth space, it tries to do exploration. Uh, if you look at the photograph, it's gone to the further end and given us a new conflict where it is, uh, not very certain about how the outcome would, would look like. And, uh, so on using this exploitation and exploration techniques.  
Chandrakala    00:07:11    Eventually Bain arrives at, uh, global maximum. Let's go to the next slide. Uh, so this is our pro HPO architecture. We've been using this opt in library, which is a Python implementation of basin algorithm. And, uh, these are the various algorithms supported, uh, TP and DP multivariate psych, and like gbm. We also have other frameworks like Hyperop and drag fly, which we are to explore. So if a user has to use this cruise hedge po, they have to interact with this. Uh, we've provided rest APIs to which we need. The user has to provide the third space, which contains, uh, the algorithm that needs to be used along with the objective function and thes, uh, with their, uh, upper bounds and lower bounds. Or if they're categorical, they can specify that. And once this is spike to the optical library, they in returns of conflict with which the user needs to run the experiment and compute the objective function and give back the result, which is again, posted to the close hbo. And based on the result, optin again, uses anon. And this, again, repeats as we already saw in the based in, uh, optimization technique, yes, until we arrive at the best configuration. So we have a small demo here, uh, which will show how to use this cruise HPO and customize it to, and to join any application. So, over to you, Seth.  
Saad    00:08:45    Yeah, uh, thank you <inaudible>. So, uh, uh, for the demo, let me just start with the, the <inaudible>. We show you that we have, it's a repository, uh, cruise in which we have multiple projects, uh, available. Uh, these are the benchmark, the actual HP application here and the UI part we are working on. So, uh, you can see these, these are the, uh, the HP application re here, and, uh, they'll focus, focus on this close demo in which we have, uh, created a script in which we run our demo application. So, uh, in that, we have a, uh, you know, um, we have, uh, a script, uh, with which we run the, uh, HPO demo application. So, uh, uh, to start with demo, what we, uh, that we just take you to that point. Yeah. So, uh, what we require initially for the demo is we require a search piece, Jason, uh, which needs to be, uh, as an input, uh, into this, uh, experiment.  
Saad    00:09:48    So, uh, just, yeah, this is the search piece, Jason, uh, which contains, uh, the experiment name, uh, experiment ID trials and everything. So, uh, the experiment, adding set of trials, as you can see, uh, uh, find optimal set of tenable values. And, uh, uh, the, the most important part of this is the objective function, which is, uh, typically an algebraic algebra expression, which can either be maximized or minimize, uh, based on the, the app, the requirement that you have. You can either maximize the throughput, minimize the cost, minimize the response time, and so on. So, uh, then we have this, as general mentioned in the slide, we are, uh, have that option of option or tp, TP option TP multi page using the TPA here. And then, uh, next step, the important part that comes is the unable part. So, s is, uh, uh, is, is, is, is nothing can have a, you know, can be of a bounded values, like an upper bound or lower down with a step, or it can be of a categorical type.  
Saad    00:10:54    So, uh, the, the bounded values can be of an integer or a double, uh, with the, with bound mentioned, uh, with, with a step value. The step is the important part here because, uh, the step is actually defining the, to, to arrive at that. Uh, let's say if you have to run, uh, your upper lower around values, like, uh, to somewhere around 10,000 or so. So the, your step can be a 0.1 or one, cause that will take a lot of time to run the experiment. So you have to define, uh, based on your, uh, uh, you know, the, uh, the upper bound value, you have to define that step as well. Then next comes, uh, for this, uh, experiment, we are using the coworkers justt benchmark. And for that we have the, uh, uh, in the tool that we have defined are the container s uh, cor S and the Java.  
Saad    00:11:44    So as you can see here, the, these are the container s here, the memory request, the CP request here, and then we have some corcus, one, like the third pool or the data sources. These are all the, uh, the bounded values. And then we have some examples of the categorical type as well, uh, which can define as a choice. It can be either a bull or it can be, uh, a set of values. Like here we have defined the garbage collection value for m here as a set of, uh, you know, a multiple options. Yeah. Uh, next, uh, next comes is, uh, let me just, uh, come back to that script that we are running. So, uh, like I told you, that search Jason, is the input that you have to provide to that. And, uh, then what we are doing is, uh, we are reading, uh, the here and then we are extracting the experiment name and the total number of trials that are present in that.  
Saad    00:12:36    And then based on that, we are starting our experiment. And, uh, here the step one will be, uh, to, to provide that such space along with your, uh, this operation. Initial operation would be, uh, the, uh, trial generate new. So this will start, uh, the experiment based on the number of trials. So see, this T trials is nothing but what we have defined here in our such in the total trials. So based on that, the, then, uh, this loop will run. Okay, so then there comes the next step to get to generate the conflict for that, uh, uh, for the first trial. So, uh, this will, uh, you know, hit using the <inaudible>, it'll hit that, uh, HP application, it'll generate the conflict. Next, what we'll do is, uh, we'll, uh, using that conflict, we'll run our, uh, our benchmark, our benchmark that, like I already mentioned, we are using the tech and part for <inaudible>.  
Saad    00:13:28    So we will, uh, uh, we will provide this, um, the conflict that HP has given us, and we'll apply that same to the, uh, the, the tech bar mark. So what we'll do is the, here this, uh, this arrangement benchmark is a separate, uh, script that we are running here. Uh, I'll, I'll not go to into that because of the lack of time. What, what, uh, in summary, what it'll do is it'll give us the, uh, if it'll, it'll apply the load, it'll capture the matrix related to that, uh, benchmark performance, and it'll calculate the, uh, the, the objective, the performance objective, and it'll give us a value. That value will, uh, get in here and we'll get the status, whether it got succeeded or fail. Based on that, uh, will proceed accordingly. Then what we'll do is the, the result that we get got back, uh, from the, uh, benchmark, uh, will again, send that, uh, uh, send that output to the HP application again. And, and, uh, what it'll do is it'll again, run the, uh, again, uh, generate the conflict based on the result that we got, uh, from the benchmark. So this will, uh, this will go on until the number of trials gets completed, and we get a final value. So let just show you by, so  
Saad    00:15:01    Hey, what we have is we have multiple option to provide the cluster light native or a doctor, uh, for this demo we are using as a native. So, but default take, even if I do define anything here, so like I mentioned, these are the has to follow. It'll first start a service. Then the, the, the second part management that we are using that will run on minicube. Okay? Uh, then it'll optimize. And then the objective values is created, the form objective. So here you can see it is starting the service here. We have two like shows in the slide. We have the rest service here, and the RPC one, total <inaudible> you're using is five. So it'll start, uh, the experiment. And as you can see, it has generated, uh, initial config here, uh, based on the s that we provided in that search space.  
Saad    00:15:54    So, uh, this will, uh, gonna take a lot of time here somewhere around, uh, for, for like, let say for five experince, I think probably 35, 45 minutes will take. So in the interest of time, what I've done is I have, uh, uh, took in the logs that we ran earlier, and we can check that and understand how it is, uh, going forward. So like I mentioned, it generated the trial, uh, conflict for the, here, if you see for the trial zero, the initial conflict, its generated this. And, uh, then we get this objective function result here. This is the status that will succeeded fail. And then same with, uh, it's sent to the HP again, and then it, uh, generated the conflict for the next trial. So this will keep go going on till, uh, the number of trials can get completed. So since we are using five plus, and we started zero, so the trial four will be the last trial.  
Saad    00:16:49    And, uh, as you can see after this, we get, uh, we get our, uh, best result out. It's the best perimeter, the best value here, the objective function value. And, uh, along in that we get the, the universe that we defined in that, uh, cell space. We get the importance, uh, as well. Also, uh, this will give us a recommended configure at the end, which will have, uh, uh, the complete, uh, what is the best requirement for least you can see, uh, here the, the best parameter that is generated here, the number question that is 3.7. This will be part of this, uh, recommended configure, uh, here.  
Saad    00:17:35    Okay, so, uh, along with this, uh, what it does is generate, uh, some charts as well to, to better analyze the output that we are getting. So the first chart that we have here is the hyper parameter importance chart. This gives us the, uh, the importance of each of the s that we have defined in our search space, Jason, uh, and, and the importance value, as you can see. So these are the other parameters that are, uh, uh, with their importance values. The next chart that we have here is, uh, the, his optimization history plot. Uh, the yx is, has having the objective function, uh, the, the objective function value, and this the, is the trials. So as you can see, uh, it started with, uh, with a Z zero trial here. And then, uh, on the third trial, we get best value here. This is defining our best value. The next chart that we have is, uh, a slide chart. It is, uh, between the objective function value and the number of trials. What it contains is, is, uh, uh, the indu, the individual unable values, uh, how the, the, the importance of those individual values change or the period of, uh, five trials. So these are the values, and what it is showing here is the, the darker the share is, uh, the higher the importance, uh, of that unable.  
Saad    00:18:55    Yeah. The next chart that we have, uh, is a parallel coordinate plot. Uh, the parallel plot contains the, it is actually showing us the relation between the objective function value and the, uh, end intervals. So, I mean, since we were using a lot of unables, there are some, somewhere, somewhere around 31 or 32, so it gets a bit, uh, confusing to understand. But, uh, later on we'll have a DP part where, you know, we can minimize this and then it'll be easier to understand. But in a nutshell, what it's showing here is, uh, the, the dark line here, it's showing the objective function, value importance for each of the team, whether its the categorical one, so the true or falls one, um, adapt the line will, uh, show it at what stage that objective function value, uh, having high importance. And the next is, uh, yeah. So these, uh, this is all generated as part of the output. Uh, let me just go back to this. Okay.  
Chandrakala    00:20:04    Yeah, I'm Sam. Uh, so you can, uh, see the genie here. So we usually compare the basing optimization with the genie because, uh, as you see your wishes, my command, so what you ask for is what you get. So if you ask for lower response time, you would get a lower response time, but it could be at, at an increased cost. So we need to be very careful when we are defining the objective function. Uh, let's move to the next slide. We have some results, uh, which we've got by running the part rest benchmark, uh, using hundred trials on a OpenShift cluster. And if you look at the, the objective function there, uh, we wanted to lower the response time, but at the same time, uh, also have high throughput and also minimize, uh, high, high tail latencies. Uh, so that is the function that we have defined, and we fixed the resources here at to four GP for the strong, and the zero trial gives us the default conflict, uh, which is at, uh, with fixit resources. And there you, we can see that the default line is 14.21 milliseconds, and with the HPO conflict out of the hundred trials, we see around 96 to 97 trial giving us 2.39 milliseconds, which is like 83%, uh, better response time. Uh, let's move to the next slide.  
Chandrakala    00:21:32    Yeah, if you look at the percentage, we got a 83% better response time. And in terms of throughput, we saw 1.3% better throughput here. We also had a collaboration with Kafka team, and, uh, uh, they had this Kafka problem where they, uh, given a fixit through put 50 pps, they wanted to determine the optimal value for these Kafka parameters that would give us the lowest, uh, latency, the end-to-end latency. Their objective function was to minimize the end-to-end latency, and they used cruise HBO for this, and they found that, uh, there was a 28% introduction. And, uh, in the end to end, they see time. Let's move to the next slide. Yeah, these are the, uh, reposts that we just saw for the cruise demos and our HBO project, and we have a cruise channel too. If you are interested, you could contribute to a project. Yeah, that is all we had for the stock. Uh, yeah, thank you.  
John    00:22:44    Um, so this is pretty, pretty cool. Um, I'm surprised with 30 parameters that with only five trials you were able to get, um, a decent result. Can you gimme some insight on how that, how come that works? So well, <laugh>,  
Chandrakala    00:23:00    It was with hundred trials. If we go back to the chat, uh, it was 31 Unables scanning across container and hotspot vm, and it was 31, uh, unables, and it was hundred trials there. If you look at the, uh, chart,  
John    00:23:20    Okay, so logically, I mean, um, it would be something like, so each, each parameter got tuned, um, into three values logically, and that was that that's enough, you know, while they're all ch changing at the same time, um, that was enough for the enough trials to, to come up with some sort of, uh, shape for the, for the surface, right?  
Chandrakala    00:23:44    Yeah. Because the way bian works is based on the previous, uh, on the result that is fighting, it gives us the next prediction. So it always predicts based on the result, and yeah, and that's how it was able to predict within a hundred trials. But yeah. Yes, I think we didn't give a around 200 trials two and, uh, we've kind of observed the response time, uh, closer to what it was that we got within hundred trials.  
Dinakar    00:24:13    I, I, I just wanted to add that if you look at the acquisition function graph, uh, sat, can you please go to that? Um, so the way that bian works is the acquisition function in your presentation. Um, so we, um, yeah, that one. Yeah. So, uh, typically basian, uh, you know, explores parts of the search space. So the pink line here is the graph that we are trying to find. So we don't know how the objective function performs. Um, I mean, like for example, it could be a combination of response time, throughput and whatever other parameters. So that's the graph that we don't know, that's the black box that we are trying to figure out. And so, uh, let's say in this particular case, it's the maximum that we are looking for, uh, the basin starts with some random values, and then it could be on any, it could land on any part of that graph.  
Dinakar    00:25:15    So once we figure out that, then it starts to figure out, you know, if it's, you know, if it, you know, it's the local maximum or the local minimum. And so it keeps exploring different parts of the earth space, uh, until it finds the global maximum in this particular case. So it so happens that sometimes, for example, it might find a local max a a minimum, in which case the, if you see the, the green graphs there at the bottom, they indicate the acquisition function, which says where, which is the most, um, you know, which is the part of the search space, which might provide the best values. So that bottoms out at points where it did not find good values. And so it will not search in that part of the search space. Whereas the parts of the search space, which were a little bit more promising, it's, you know, it, it, you know, exploits them as it is called.  
Dinakar    00:26:12    So, so it, it starts exploring newer parts of the search space, um, where you know, where it has not looked, and then depending on the results, it, uh, you know, continues to exploit them if it finds that they are promising. But if it finds that they're not so promising, then it, you know, uh, moves on to a different parts of the search space and so on. So that's essentially, uh, and, and of course all the different algorithms that we were looking at, I mean, tpe, TP motivated and so on, they have, uh, different efficiencies in terms of how quickly they can arrive at the right values. Uh, yeah, of course there's nothing that can arrive within five trials. It'll take a pretty long time. Um, but there are a few of them, which we have not explored really, like, you know, things like XG Boost and uh, and so on that do actually shorten the amount of time that it takes for you to arrive at the, uh, uh, you know, the best possible values. Essentially,  
John    00:27:11    If you go back to the, the chart where it had all the trials on the bottom and then the, the score was near the end of the slideshow,  
Dinakar    00:27:20    The slides slide  
John    00:27:25    That, oh, keep going. Nope, way at the end. That one probably. Yeah. So it looks like every once in a while it, it goes, it, you know, does like five or 10 10 that, that are looking at the local minimum, but then it does a jump. Is that part of the algorithm?  
Dinakar    00:27:43    Yeah, that's the exploring versus exploitation that I was talking about. So the exploration, so the ones where you see values, so here lower is better in this particular graph, uh, can you hit on slideshow? So it'll be easier for, um, so, uh, the values lower are better. And so when it tries to, uh, explore a newer part of the third space, so that's when it finds that the response time goes up because, you know, it may find something that's not so good, then it starts to look adjacent to that and then find something that is good and starts exploiting that. So that's why it typically, it does something like, um, around, uh, you know, within, let's say if you consider 10 trials around the first two or three of them would be the exploration part and then comes the exploitation part. And so that sort of repeats all through the hundred trials that you see there.  
John    00:28:38    Thank you. 
